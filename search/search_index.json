{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Running AI/ML Workloads on NAISS Systems","text":"<p>This website is currently under development. The training session will be part of the NAISS Intro Week: https://naiss-training.github.io/NAISS-intro-week/AIML/.</p>"},{"location":"presentation/","title":"Running AI/ML workloads on NAISS systems","text":""},{"location":"presentation/#scope","title":"Scope","text":"<ul> <li>Will be covered:<ul> <li>Introduction for running Deep Learning workloads on the main NAISS AI/ML resource</li> </ul> </li> <li>Will not be covered:<ul> <li>A general introduction to machine learning</li> <li>Running classical ML or GOFAI</li> <li>General HPC intro</li> </ul> </li> </ul>"},{"location":"presentation/#naiss-gpu-resources-overview","title":"NAISS GPU resources overview","text":"<ol> <li>Alvis (End-of-life 2026-06-30)<ul> <li>NVIDIA GPUs: 332 A40s, 318 A100s, 160 T4s, 44 V100s</li> <li>Only for AI/ML</li> </ul> </li> <li>Arrhenius (to be in operation spring 2026)<ul> <li>1528 NVIDIA GH200s</li> </ul> </li> <li>Dardel<ul> <li>248 AMD MI250X</li> </ul> </li> <li>Bianca<ul> <li>20 NVIDIA A100s</li> <li>Only for sensitive data</li> </ul> </li> </ol>"},{"location":"presentation/#alvis-specifics","title":"Alvis specifics","text":"<ul> <li>What is potentially different on Alvis?</li> <li>See extended version at Alvis introduction material</li> </ul>"},{"location":"presentation/#connecting-firewall","title":"Connecting - Firewall","text":"<ul> <li>Firewall limits connections to within   SUNET</li> <li>Use a VPN if needed</li> </ul>"},{"location":"presentation/#log-in-nodes","title":"Log-in nodes","text":"<ul> <li><code>alvis1.c3se.chalmers.se</code> has 4 T4 GPUs for light testing and debugging</li> <li><code>alvis2.c3se.chalmers.se</code> is dedicated data transfer node</li> <li>Will be restarted from time to time</li> <li>Login nodes are shared resources for all users:<ul> <li>don't run jobs here,</li> <li>don't use up too much memory,</li> <li>preparing jobs and</li> <li>light testing/debugging is fine</li> </ul> </li> </ul>"},{"location":"presentation/#ssh-secure-shell","title":"SSH - Secure Shell","text":"<ul> <li><code>ssh &lt;CID&gt;@alvis1.c3se.chalmers.se</code>, <code>ssh &lt;CID&gt;@alvis2.c3se.chalmers.se</code></li> <li>Gives command line access to do anything you could possibly need</li> <li>If used frequently you can set-up a password protected   SSH-key for convenience</li> </ul>"},{"location":"presentation/#alvis-open-ondemand-portal","title":"Alvis Open OnDemand portal","text":"<ul> <li>https://alvis.c3se.chalmers.se</li> <li>Browse files and see disk and file quota</li> <li>Launch interactive apps on compute nodes<ul> <li>Desktop</li> <li>Jupyter notebooks</li> <li>MATLAB proxy</li> <li>RStudio</li> <li>VSCode</li> </ul> </li> <li>Launch apps on log-in nodes<ul> <li>TensorBoard</li> <li>Desktop</li> </ul> </li> <li>See our documentation for more</li> </ul>"},{"location":"presentation/#remote-desktop","title":"Remote desktop","text":"<ul> <li>RDP-based remote desktop solution on shared login nodes (use portal for   heavier interactive jobs)</li> <li>In-house-developed web client:<ul> <li>https://alvis1.c3se.chalmers.se</li> <li>https://alvis2.c3se.chalmers.se</li> </ul> </li> <li>Can also be accessed via desktop clients such as Windows Remote Desktop   Connection (Windows), FreeRDP/Remmina/krdc (Linux) and Windows App (Mac) at    and  (standard port 3389). <li>Desktop clients tend to offer better quality and more ergonomic experiences.</li> <li>See   the documentation   for more details</li>"},{"location":"presentation/#files-and-storage","title":"Files and Storage","text":"<ul> <li>Cephyr <code>/cephyr/</code>, and Mimer <code>/mimer/</code> are parallel filesytems, accessible   from all nodes</li> <li>Backed up home directory at <code>/cephyr/users/&lt;CID&gt;/Alvis</code>   (alt. use <code>~</code>)</li> <li>Project storage at <code>/mimer/NOBACKUP/groups/&lt;storage-name&gt;</code></li> <li>The <code>C3SE_quota</code> shows you all your centre storage areas, usage and quotas.<ul> <li>On Cephyr see file usage with <code>where-are-my-files</code></li> </ul> </li> <li>File-IO is usually the limiting factor on parallel filesystems</li> <li>If you can deal with a few large files instead of many small, that is   preferable</li> </ul>"},{"location":"presentation/#datasets","title":"Datasets","text":"<ul> <li>When allowed, we provide popular datasets at <code>/mimer/NOBACKUP/Datasets/</code></li> <li>To request an additional dataset, do so through the support form</li> <li>It is your responsibility to make sure you comply with any licenses and limitations<ul> <li>In all cases only for non-commercial research applications</li> <li>Citation often needed</li> </ul> </li> <li>Read more on the dataset page and/or the respective README files</li> </ul>"},{"location":"presentation/#software","title":"Software","text":"<ul> <li>Containers through Apptainer</li> <li>Optimized software in [modules]<ul> <li>Flat module scheme, load modules directly</li> </ul> </li> <li>Read the Python instructions for installing your own Python packages</li> </ul>"},{"location":"presentation/#gpu-hardware-details","title":"GPU hardware details","text":"#GPUs GPUs Capability CPU Note 44 V100 7.0 Skylake 160 T4 7.5 Skylake 332 A40 8.6 Icelake No IB 296 A100 8.0 Icelake Fast Mimer 32 A100fat 8.0 Icelake Fast Mimer"},{"location":"presentation/#slurm-specifics","title":"SLURM specifics","text":"<ul> <li>Main allocatable resource is <code>--gpus-per-node=&lt;GPU type&gt;:&lt;no. gpus&gt;</code><ul> <li>e.g. <code>#SBATCH --gpus-per-node=A40:1</code></li> </ul> </li> <li>Cores and memory are allocated proportional to number of GPUs and related node type</li> <li>Maximum 7 days walltime<ul> <li>Use checkpointing for longer runs</li> </ul> </li> <li>Jobs that don't use allocated GPUs may be automatically cancelled</li> </ul>"},{"location":"presentation/#gpu-cost-on-alvis","title":"GPU cost on Alvis","text":"Type VRAM System memory per GPU CPU cores per GPU Cost T4 16GB 72 or 192 GB 4 0.35 A40 48GB 64 GB 16 1 V100 32GB 192 or 384 GB 8 1.31 A100 40GB 64 or 128 GB 16 1.84 A100fat 80GB 256 GB 16 2.2 <ul> <li>Example: using 2xT4 GPUs for 10 hours costs 7 \"GPU hours\" (2 x 0.35 x 10).</li> <li>The cost reflects the actual price of the hardware (normalised against an A40   node/GPU).</li> </ul>"},{"location":"presentation/#monitoring-tools","title":"Monitoring tools","text":"<ul> <li>You can SSH to nodes where you have an ongoing job<ul> <li>From where you can use CLI tools like <code>htop</code>, <code>nvidia-smi</code>, <code>nvtop</code>, ...</li> </ul> </li> <li>Use <code>job_stats.py &lt;JOBID&gt;</code> to view graphs of usage</li> <li><code>jobinfo -s</code> can be used to get a summary of currently available resources</li> </ul>"},{"location":"presentation/#running-ml","title":"Running ML","text":"<ul> <li>How you run on GPUs with PyTorch and TensorFlow</li> </ul>"},{"location":"presentation/#pytorch","title":"PyTorch","text":"<ul> <li>Move tensors or models to the GPU \"by hand\"</li> </ul> <pre><code>import torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\na = torch.Tensor([1, 1, 2, 3]).to(device)\n</code></pre>"},{"location":"presentation/#tensorflow","title":"TensorFlow","text":"<ul> <li>Automatically tries to use a single GPU</li> <li>Will also pre-allocate GPU memory, obfuscating actual memory usage to external monitoring tools</li> <li>https://www.tensorflow.org/guide/gpu</li> </ul> <pre><code>import tensorflow as tf\n\nprint(tf.config.list_physical_devices(\"GPU\"))\n</code></pre>"},{"location":"presentation/#demos","title":"Demos","text":"<ul> <li>PyTorch: TBD</li> <li>TensorFlow: TBD</li> </ul>"},{"location":"presentation/#performance-and-gpus","title":"Performance and GPUs","text":"<ul> <li>Floating point precision and GPU performance</li> </ul>"},{"location":"presentation/#performance-and-parallel-filesystems","title":"Performance and parallel filesystems","text":"<ul> <li>Performance considerations for data loading on parallel filesystems</li> </ul>"},{"location":"presentation/#profiling","title":"Profiling","text":"<ul> <li>Profiling your ML workload</li> </ul>"},{"location":"presentation/#multi-gpu-parallelism","title":"Multi-GPU parallelism","text":"<ul> <li>Multi-GPU parallelism<ul> <li>Conceptual overview</li> <li>Data Parallellism</li> <li>Fully Sharded Data Parallel</li> </ul> </li> </ul>"},{"location":"presentation/#basic-llm-inference","title":"Basic LLM inference","text":"<ul> <li>vLLM</li> </ul>"}]}